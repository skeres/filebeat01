# Poc of FILEBEAT

We will use :  
- A spring boot application to generate sp√©cific logs
- Elasticsearch and Kibana to store log datas and create dashboard
- Filebeat to ship log datas and push them to Elasticseatch

## Run Spring boot Maven project to generate logs
Project path : /home/stephane/projets/projets_gitlabCI/gitlabci_01  
On **GITLAB** : [repository ](https://gitlab.com/skeres/gitlabci_01.git)  



## Run Elasticsearch and Kibana using Docker
Project path : /home/stephane/projets/projets_elk/filebeat01  
On github : here !

**Create docker volume if not already exists**  
```
docker volume create --name elasticsearch-filebeat
```

**Create docker network if not already exists**  
```
docker network create --driver=bridge --ip-range=172.28.1.0/24 --subnet=172.28.0.0/16 --gateway=172.28.1.254 my_custom_network
```

**Run Elasticsearch and Kibana**  
```
docker-compose -f docker-compose.yaml up -d
```
Or just :  
```
docker-compose up -d
```

**Test Elasticsearch health and access**  
```
curl -XGET "http://localhost:9200/_cluster/health?pretty"
```   

or in a browser :  
```
http://localhost:9200/
```

**Test Kibana access**  
```
localhost:5601/
```

**Stop container without removing them**  
```
docker-compose -f docker-compose.yaml stop
```
Or just :   
```
docker-compose stop
```

** Restart container if they already exist**  
```
docker start elastic-filebeat && docker start kibana-filebeat
```

**Stop and remove caontainers
```
docker-compose down
```

## Install Filebeat on your host
source  :https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-installation-configuration.html  

Project path : /home/stephane/filebeat/filebeat-7.17.9-linux-x86_64
On github : none  

We're gonna use Filebeat as a process, not like an agent. but the result is the same as if it was running as an agent on a host  

$ mkdir filebeat && cd filebeat  
$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.17.9-linux-x86_64.tar.gz  
$ tar xzvf filebeat-7.17.9-linux-x86_64.tar.gz  
$ cd filebeat-7.17.9-linux-x86_64  
$ cp filebeat.yml filebeat.yml.backup_initial_20230207_08h30  
$ cat filebeat.yml  

## How to use Filebeat
sources :   
Filebeat : https://www.youtube.com/watch?v=ykuw1piMGa4&t=2123s  
kibana : https://www.youtube.com/watch?v=e1299MWyr98  


**To see a list of available modules**  
$ ./filebeat modules list  

**Test file filebeat.yml**  
$ ./filebeat test config  

**Test output and Elastcisearch access**  
$ /filebeat test output

**Set up :**  
Set up should only be run once as changes are made in filebeat.yml   
filebeat will contact the output and start creating all resources needed to ship datas in elasticsearch,   
like indexes, indexes patterns and so on. It can take up to 1 minute to proceed  
$ vi filebeat.yml and change input configuration to true   
  enabled: true   

$ ./filebeat setup  
result :  
*Overwriting ILM policy is disabled. Set `setup.ilm.overwrite: true` for enabling.*  
*Index setup finished*   
*Loading dashboards (Kibana must be running and reachable)*  
*Loaded dashboards*  
*Setting up ML using setup --machine-learning is going to be removed in 8.0.0. Please use the ML app instead.*  
*See more: https://www.elastic.co/guide/en/machine-learning/current/index.html*  
*It is not possble to load ML jobs into an Elasticsearch 8.0.0 or newer using the Beat.*  
*Loaded machine learning job configurations*  
*Loaded Ingest pipelines*  

**Execute filebeat, and ctrl+c to cancel the process**
$ ./filebeat -e  



## Customize log parsing for data
At this part of the POC, we're gonna customize Filebeat to read the log file generated by   
the spring boot application above.  
The log file is at this location : gitlabci_01/logs/application_logback.log  

Launch applicatio and go to your browser at those addresses :
http://localhost:8080/date  
http://localhost:8080/hello  

Refresh the browser, and go to see the log content in gitlabci_01/logs/application_logback.log

** Data format log generated with spring boot looks like this  
023-02-07 16:10:44 [http-nio-8080-exec-8] DEBUG : project gitlabci|controller|date|2023-02-07 16:10:44   
2023-02-07 16:10:45 [http-nio-8080-exec-10] DEBUG : project gitlabci|controller|date|2023-02-07 16:10:45  
2023-02-07 16:10:54 [http-nio-8080-exec-1] DEBUG : project gitlabci|controller|hello|2023-02-07 16:10:54  

*In Kibana, using wizard "upload file" with machine learnig/data visualizer we got :*  
%{TIMESTAMP_ISO8601:timestamp} \[.*?\] %{LOGLEVEL:loglevel} : .*? .*?\|.*?\|.*?\|%{TIMESTAMP_ISO8601:extra_timestamp}

*We override it and customize Grock like this*  
%{TIMESTAMP_ISO8601:timestamp} \[%{GREEDYDATA:protocol}\] %{LOGLEVEL:loglevel} : %{GREEDYDATA:project}\|%{GREEDYDATA:classeName}\|%{GREEDYDATA:controllerName}\|%{TIMESTAMP_ISO8601:extra_timestamp}  

*Continue wizard using "Import" button*  
set an index name : springboot  
clicking "advanced" tab, you can see index mapping and Ingest pipeline that elasticsearch will use to import  datas :   

**Mappings :**  
```
{
  "properties": {
    "@timestamp": {
      "type": "date"
    },
    "classeName": {
      "type": "keyword"
    },
    "controllerName": {
      "type": "keyword"
    },
    "extra_timestamp": {
      "type": "date",
      "format": "yyyy-MM-dd HH:mm:ss"
    },
    "loglevel": {
      "type": "keyword"
    },
    "message": {
      "type": "text"
    },
    "project": {
      "type": "keyword"
    },
    "protocol": {
      "type": "keyword"
    }
  }
}
```  

**Ingest Pipeline :**   
```
  "description": "Ingest pipeline created by text structure finder",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": [
          "%{TIMESTAMP_ISO8601:timestamp} \\[%{GREEDYDATA:protocol}\\] %{LOGLEVEL:loglevel} : %{GREEDYDATA:project}\\|%{GREEDYDATA:classeName}\\|%{GREEDYDATA:controllerName}\\|%{TIMESTAMP_ISO8601:extra_timestamp}"
        ]
      }
    },
    {
      "date": {
        "field": "timestamp",
        "timezone": "{{ event.timezone }}",
        "formats": [
          "yyyy-MM-dd HH:mm:ss"
        ]
      }
    },
    {
      "remove": {
        "field": "timestamp"
      }
    }
  ]
}

```  


*A lot of thing then happened :*  
file processed  
Index created  
Ingest pipeline created  
Data uploaded  
Index pattern created  


Go to "ingest pipeline" to see springboot-pipeline : 
```
[
  {
    "grok": {
      "field": "message",
      "patterns": [
        "%{TIMESTAMP_ISO8601:timestamp} \\[%{GREEDYDATA:protocol}\\] %{LOGLEVEL:loglevel} : %{GREEDYDATA:project}\\|%{GREEDYDATA:classeName}\\|%{GREEDYDATA:controllerName}\\|%{TIMESTAMP_ISO8601:extra_timestamp}"
      ]
    }
  },
  {
    "date": {
      "field": "timestamp",
      "timezone": "Europe/Paris",
      "formats": [
        "yyyy-MM-dd HH:mm:ss"
      ]
    }
  },
  {
    "remove": {
      "field": "timestamp"
    }
  }
]
```

## Create Filebeat configuration file
Click on button "Create Filebeat configuration file" and copy/paste content in a new file called : 
specific-filebeat-conf.yaml

**change/adapt the content to your configuration**  
- '<add path to your files here>'   by  - /home/stephane/projets/projets_gitlabCI/gitlabci_01/logs/*.log
 hosts: ["<es_url>"]  by  hosts: ["localhost:9200"]

The final content is below :   

```
filebeat.inputs:

# filestream is an input for collecting log messages from files.
- type: filestream

  # Change to true to enable this input configuration.
  enabled: true
  
  # Unique ID among all inputs, an ID is required.
  id: my-filestream-id
  
  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /home/stephane/projets/projets_gitlabCI/gitlabci_01/logs/*.log
    #- c:\other\logs\*
  multiline:
    pattern: '^\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}'
    match: after
    negate: true

processors:
- add_locale: ~

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "springboot"
  pipeline: "springboot-pipeline"

setup:
  template.enabled: false
  ilm.enabled: false
  
# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat",
# "publisher", "service".
#logging.selectors: ["*"]
```


## Run filebeat again

$ ./filebeat -c specific-filebeat-conf.yaml test config  
$ ./filebeat -c specific-filebeat-conf.yaml test output  
$ ./filebeat -c specific-filebeat-conf.yaml setup  

**Tip** : rm -rf data directory before launching filebeat if you work on existing datas  
Execute :  
$ ./filebeat -c specific-filebeat-conf.yaml -e  
**Tip** : -e is optional and sends output to standard error instead of the configured log output )  
  
Go to Menu analytic/discover and select the good range of date to see your datas; ALL DONe !!!  
** Tip ** : to stop filebeat, type ctrl+c  

